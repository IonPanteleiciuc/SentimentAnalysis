{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1efe9e",
   "metadata": {},
   "source": [
    "## Authors: Guerlain Messin & Ion Panteleiciuc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b17c85",
   "metadata": {},
   "source": [
    "# Lab: Semantic search on Question Answering datasets\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "1. Explore and understand the **S**tanford **Qu**estion **A**nswering **D**ataset [Squad](https://aclanthology.org/D16-1264/) dataset and the associated task.  \n",
    "2. Adapt this dataset for a *local* semantic search task and propose an appropriate evaluation metric:\n",
    "    - Implement a simple baseline based on **TF-IDF**.\n",
    "    - Use a pre-trained transformer-based model, and fine-tune it.\n",
    "3. Test these approaches on the [CommonSense QA](https://aclanthology.org/N19-1421/) dataset. \n",
    "4. Adapt these approaches for a *global* semantic search task on the [WikiQA](https://aclanthology.org/D15-1237/) dataset for open domain question answering.\n",
    "5. **Bonus** (Optional) Apply a model (any, as long as it's running) to the original Squad QA task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "83b8ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from evaluate import load\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer, losses, SentencesDataset, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "import faiss\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de1dc6",
   "metadata": {},
   "source": [
    "## 1 - The SQuAD dataset\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Questions:</div>\n",
    "\n",
    "1. Load the dataset **SQuAD** - for example, using the [```dataset``` package](https://huggingface.co/docs/datasets/index) from Huggingface and loading the dataset ```'squad'```. You can also explore it using the [website](https://rajpurkar.github.io/SQuAD-explorer/). \n",
    "2. Look at the metrics used to evaluate models on the dataset. You can also load the metric ```'squad'``` from the [```evaluate``` package](https://huggingface.co/docs/evaluate/index) from Huggingface. \n",
    "3. Explain succintly - and in your own words - what is the task: how could we use a model to solve it ? Treat the case of encoder models adapted to *classification tasks* and encoder-decoder models adapted to *text generation*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0cf2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/Guerlain/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba59254d39e549fb8e034d960a7b8a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/Guerlain/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-10de9997c4b83f65.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/Guerlain/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-f3a033b6ac26514f.arrow\n"
     ]
    }
   ],
   "source": [
    "squad_dataset = load_dataset('squad')\n",
    "metric = load('squad')\n",
    "\n",
    "train_dataset = squad_dataset['train'].shuffle(seed=42).select([i for i in range(1000)])\n",
    "valid_dataset = squad_dataset['validation'].shuffle(seed=42).select([i for i in range(1000)])\n",
    "\n",
    "squad_dataset = DatasetDict({'train': train_dataset, 'validation': valid_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9c7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_dataset)\n",
    "df_valid = pd.DataFrame(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd4da9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>573173d8497a881900248f0c</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>The Pew Forum on Religion &amp; Public Life ranks ...</td>\n",
       "      <td>What percentage of Egyptians polled support de...</td>\n",
       "      <td>{'text': ['84%'], 'answer_start': [468]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57277e815951b619008f8b52</td>\n",
       "      <td>Ann_Arbor,_Michigan</td>\n",
       "      <td>The Ann Arbor Hands-On Museum is located in a ...</td>\n",
       "      <td>Ann Arbor ranks 1st among what goods sold?</td>\n",
       "      <td>{'text': ['books'], 'answer_start': [402]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5727e2483acd2414000deef0</td>\n",
       "      <td>Rule_of_law</td>\n",
       "      <td>One important aspect of the rule-of-law initia...</td>\n",
       "      <td>In developing countries, who makes most of the...</td>\n",
       "      <td>{'text': ['the executive'], 'answer_start': [6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5728f5716aef0514001548cc</td>\n",
       "      <td>Samurai</td>\n",
       "      <td>In December 1547, Francis was in Malacca (Mala...</td>\n",
       "      <td>Who impressed Xavier by taking notes in church?</td>\n",
       "      <td>{'text': ['Anjiro'], 'answer_start': [160]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>572826002ca10214002d9f16</td>\n",
       "      <td>Group_(mathematics)</td>\n",
       "      <td>Groups are also applied in many other mathemat...</td>\n",
       "      <td>What represents elements of the fundamental gr...</td>\n",
       "      <td>{'text': ['loops'], 'answer_start': [489]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                title  \\\n",
       "0  573173d8497a881900248f0c                Egypt   \n",
       "1  57277e815951b619008f8b52  Ann_Arbor,_Michigan   \n",
       "2  5727e2483acd2414000deef0          Rule_of_law   \n",
       "3  5728f5716aef0514001548cc              Samurai   \n",
       "4  572826002ca10214002d9f16  Group_(mathematics)   \n",
       "\n",
       "                                             context  \\\n",
       "0  The Pew Forum on Religion & Public Life ranks ...   \n",
       "1  The Ann Arbor Hands-On Museum is located in a ...   \n",
       "2  One important aspect of the rule-of-law initia...   \n",
       "3  In December 1547, Francis was in Malacca (Mala...   \n",
       "4  Groups are also applied in many other mathemat...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What percentage of Egyptians polled support de...   \n",
       "1         Ann Arbor ranks 1st among what goods sold?   \n",
       "2  In developing countries, who makes most of the...   \n",
       "3    Who impressed Xavier by taking notes in church?   \n",
       "4  What represents elements of the fundamental gr...   \n",
       "\n",
       "                                             answers  \n",
       "0           {'text': ['84%'], 'answer_start': [468]}  \n",
       "1         {'text': ['books'], 'answer_start': [402]}  \n",
       "2  {'text': ['the executive'], 'answer_start': [6...  \n",
       "3        {'text': ['Anjiro'], 'answer_start': [160]}  \n",
       "4         {'text': ['loops'], 'answer_start': [489]}  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a50903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gaddafi was a very private individual, who described himself as a \"simple revolutionary\" and \"pious Muslim\" called upon by Allah to continue Nasser\\'s work. Reporter Mirella Bianco found that his friends considered him particularly loyal and generous, and asserted that he adored children. She was told by Gaddafi\\'s father that even as a child he had been \"always serious, even taciturn\", a trait he also exhibited in adulthood. His father said that he was courageous, intelligent, pious, and family oriented.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.context[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c015a97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whose efforts did Gaddafi see himself as continuing?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[40].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77717c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Nasser'], 'answer_start': [141]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[40].answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03866bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572759665951b619008f8884</td>\n",
       "      <td>Private_school</td>\n",
       "      <td>Private schooling in the United States has bee...</td>\n",
       "      <td>In what year did Massachusetts first require c...</td>\n",
       "      <td>{'text': ['1852', '1852', '1852'], 'answer_sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57296de03f37b3190047839e</td>\n",
       "      <td>Chloroplast</td>\n",
       "      <td>The chloroplast membranes sometimes protrude o...</td>\n",
       "      <td>When were stromules discovered?</td>\n",
       "      <td>{'text': ['1962', '1962', '1962'], 'answer_sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5726d4a45951b619008f7f6c</td>\n",
       "      <td>Victoria_and_Albert_Museum</td>\n",
       "      <td>Not only the work of British artists and craft...</td>\n",
       "      <td>Which artist who had a major influence on the ...</td>\n",
       "      <td>{'text': ['Horace Walpole', 'Horace Walpole', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>572843304b864d1900164848</td>\n",
       "      <td>University_of_Chicago</td>\n",
       "      <td>In the 1890s, the University of Chicago, fearf...</td>\n",
       "      <td>In 1890, who did the university decide to team...</td>\n",
       "      <td>{'text': ['several regional colleges and unive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56d729180d65d21400198427</td>\n",
       "      <td>Super_Bowl_50</td>\n",
       "      <td>After a punt from both teams, Carolina got on ...</td>\n",
       "      <td>Who got a touchdown making the score 10-7?</td>\n",
       "      <td>{'text': ['Jonathan Stewart', 'Jonathan Stewar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                       title  \\\n",
       "0  572759665951b619008f8884              Private_school   \n",
       "1  57296de03f37b3190047839e                 Chloroplast   \n",
       "2  5726d4a45951b619008f7f6c  Victoria_and_Albert_Museum   \n",
       "3  572843304b864d1900164848       University_of_Chicago   \n",
       "4  56d729180d65d21400198427               Super_Bowl_50   \n",
       "\n",
       "                                             context  \\\n",
       "0  Private schooling in the United States has bee...   \n",
       "1  The chloroplast membranes sometimes protrude o...   \n",
       "2  Not only the work of British artists and craft...   \n",
       "3  In the 1890s, the University of Chicago, fearf...   \n",
       "4  After a punt from both teams, Carolina got on ...   \n",
       "\n",
       "                                            question  \\\n",
       "0  In what year did Massachusetts first require c...   \n",
       "1                    When were stromules discovered?   \n",
       "2  Which artist who had a major influence on the ...   \n",
       "3  In 1890, who did the university decide to team...   \n",
       "4         Who got a touchdown making the score 10-7?   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['1852', '1852', '1852'], 'answer_sta...  \n",
       "1  {'text': ['1962', '1962', '1962'], 'answer_sta...  \n",
       "2  {'text': ['Horace Walpole', 'Horace Walpole', ...  \n",
       "3  {'text': ['several regional colleges and unive...  \n",
       "4  {'text': ['Jonathan Stewart', 'Jonathan Stewar...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88e1d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answers\n",
       "1    1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.answers.apply(lambda x: len(x['text'])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57afd5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answers\n",
       "3    803\n",
       "5    120\n",
       "4     59\n",
       "2     16\n",
       "6      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.answers.apply(lambda x: len(x['text'])).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab9d41",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "The task in the Stanford Question Answering Dataset (SQuAD) is to develop models that can accurately answer questions based on a given passage of text. In SQuAD, answers can be any sequence of words within the provided text.</br>\n",
    "\n",
    "- For <b>encoder models</b> adapted to classification tasks, the goal is to train a model that takes both the question and the passage as input and predicts the span of text that forms the correct answer. The model's encoder processes the input text and produces a representation that is used for classification, identifying the start and end positions of the answer within the passage.\n",
    "\n",
    "- For <b>encoder-decoder models</b> adapted to text generation, the focus is on generating coherent and contextually relevant answers. The encoder processes the input question and passage, while the decoder generates a sequence of words constituting the answer. This type of model is well-suited for tasks where the answer is not a direct span of text but requires a more expressive generation of language.\n",
    "\n",
    "In summary, for encoder models adapted to classification, the task is to predict answer spans, while for encoder-decoder models adapted to text generation, the task is to generate answers in a more flexible manner. Both approaches aim to make the model understand and process the context provided by the passage to accurately respond to questions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba300531",
   "metadata": {},
   "source": [
    "## 2 - Design a *local* semantic search from squad\n",
    "\n",
    "This taks is a little complicated to implement. Let us simplify squad to be a **semantic search** task !\n",
    "We will divide the context containing the answer into several pieces, and ask a model to find which one contains the answer **by vectorizing the question and each piece** and trying to look for the most relevant piece using **cosine similarity** between the vectors, making it a fairly simple task.\n",
    "\n",
    "\n",
    "For example, the following question of the dataset:\n",
    "\n",
    "```python\n",
    "'Which NFL team represented the AFC at Super Bowl 50?'\n",
    "```\n",
    "\n",
    "with the answer:\n",
    "\n",
    "```python\n",
    "'Denver Broncos'\n",
    "```\n",
    "\n",
    "We could divide the corresponding ```'context'``` into the following list:\n",
    "\n",
    "```python\n",
    "['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos d',\n",
    " \"efeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Franc\",\n",
    " 'isco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending',\n",
    " ' the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.']\n",
    "```\n",
    "\n",
    "and indicate the location of the answer as:\n",
    "\n",
    "```python\n",
    "label = [1, 0, 0, 0]\n",
    "```\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "At first, we won't do any training: you should work with the ```validation``` part of the dataset. Be careful, there may be several good answers ! Propose a scheme to divide the context into pieces and to label each piece as containing the answer or not. How do we evaluate for this task - would simple accuracy suffice ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f311f278",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "- <b>1. Divide the context into pieces:</b> We can divide the context into chunks, considering each chunk as a potential candidate for containing the answer. A simple approach would be to use sliding windows to create overlapping or non-overlapping chunks.\n",
    "\n",
    "- <b>2. Label each piece:</b> Label each chunk based on whether it contains the answer or not. If a chunk contains the answer, label it as 1; otherwise, label it as 0.\n",
    "\n",
    "- <b>3. Vectorize the question and each piece:</b> Use vector embeddings for the question and each chunk of the context (with BERT for example).\n",
    "\n",
    "- <b>4. Calculate cosine similarity:</b> Compute the cosine similarity between the vector representation of the question and each chunk. The chunk with the highest cosine similarity is considered the most relevant.\n",
    "\n",
    "- <b>5. Evaluate the model:</b> For our evaluation, we could have used the dedicated squad metric to evaluate the quality of our model. This metric takes into account the exact matchs and the f1-score of our propositon. But here, we will just use classic metrics (precision, recall, and f1-score).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160c68f",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "For efficient processing, you can use the ```map``` method associated to the dataset. It can create a new feature for each example. In this case, you can create a new feature containing the context divided into pieces, and a new feature containing labels for if the pieces contain the answer. You can also use it for your evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49041577",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd01726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_and_label(dataset):\n",
    "    doc = nlp(dataset['context'])\n",
    "    cut_context = [sent.text for sent in doc.sents]\n",
    "    labels = [1 if dataset['answers']['text'][0] in sentence else 0 for sentence in cut_context]\n",
    "\n",
    "    return {'cut_context': cut_context, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4692194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/Guerlain/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-1750a4f86fb84402.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e653783fc9694fc88b7d98a369c12b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad_dataset = squad_dataset.map(cut_and_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "728527ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '573173d8497a881900248f0c',\n",
       " 'title': 'Egypt',\n",
       " 'context': 'The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.',\n",
       " 'question': 'What percentage of Egyptians polled support death penalty for those leaving Islam?',\n",
       " 'answers': {'text': ['84%'], 'answer_start': [468]},\n",
       " 'cut_context': ['The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom.',\n",
       "  'The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government.',\n",
       "  'According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery.'],\n",
       " 'labels': [0, 0, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0d5ef",
   "metadata": {},
   "source": [
    "### 2.1 - Local search: Independant Tf-idf representations\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "Implement a function that will for each example:\n",
    "- Create a tf-idf ```vectorizer``` from all the text in the question and context. \n",
    "- Create tf-idf representations for the question and the pieces of the context,\n",
    "- Find the representation the closest to the question among the pieces.\n",
    "\n",
    "Then, evaluate the method !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a523c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_predict_label(dataset):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit([dataset['context'] + ' ' + dataset['question']])\n",
    "\n",
    "    tfidf_vectorized_cut_contexts = vectorizer.transform(dataset['cut_context']).toarray()\n",
    "    tfidf_vectorized_questions = vectorizer.transform([dataset['question']]).toarray()\n",
    "\n",
    "    similarities = [cosine_similarity(tfidf_vectorized_questions, vectorized_cut_context.reshape(1, -1))[0][0] for vectorized_cut_context in tfidf_vectorized_cut_contexts]\n",
    "\n",
    "    index_with_most_similarity = np.argmax(similarities)\n",
    "\n",
    "    predicted_labels = [1 if i == index_with_most_similarity else 0 for i in range(len(similarities))]\n",
    "\n",
    "    return {'index_with_most_similarity': index_with_most_similarity, 'predicted_labels': predicted_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1a3ac3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732b63385c794797b2cc2203d00bb845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_squad_dataset = squad_dataset['validation'].map(tfidf_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c34a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 65.7%\n",
      "recall: 73.8%\n",
      "f1 score: 69.5%\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = tfidf_squad_dataset['predicted_labels']\n",
    "labels = tfidf_squad_dataset['labels']\n",
    "\n",
    "flat_predicted_labels = [predicted_label for predicted_unit in predicted_labels for predicted_label in predicted_unit]\n",
    "flat_labels = [label for label_unit in labels for label in label_unit]\n",
    "\n",
    "score_precision = precision_score(flat_predicted_labels, flat_labels)\n",
    "score_recall = recall_score(flat_predicted_labels, flat_labels)\n",
    "score_f1 = f1_score(flat_predicted_labels, flat_labels)\n",
    "\n",
    "print(f'precision: {100*score_precision:.1f}%')\n",
    "print(f'recall: {100*score_recall:.1f}%')\n",
    "print(f'f1 score: {100*score_f1:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44498817",
   "metadata": {},
   "source": [
    "### 2.2 - Local search: Pre-trained sentence representations transformer-based model\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "\n",
    "Reproduce the same process using a pre-trained transformer model. You can use a model that you will find on huggingface. You can also look into the [```SentenceTransformer``` library](https://www.sbert.net/), dedicated to represent documents. Also:\n",
    "- Try to verify if the model has been trained on SQuAD !\n",
    "- Fine-tune the model (at least a little) to check that it improves results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc9c97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('nq-distilbert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4914be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_vectorizer_predict_label(dataset):\n",
    "\n",
    "    bert_encoded_cut_contexts = model.encode(dataset['cut_context'])\n",
    "    bert_encoded_questions = model.encode(dataset['question'])\n",
    "\n",
    "    similarities = [cosine_similarity([bert_encoded_questions], [encoded_cut_context])[0][0] for encoded_cut_context in bert_encoded_cut_contexts]\n",
    "\n",
    "    index_with_most_similarity = np.argmax(similarities)\n",
    "\n",
    "    predicted_labels = [1 if i == index_with_most_similarity else 0 for i in range(len(similarities))]\n",
    "\n",
    "    return {'index_with_most_similarity': index_with_most_similarity, 'predicted_labels': predicted_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e490004d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05142a6581c495b9b7a5a457e6768a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_vectorizer_squad_dataset = squad_dataset['validation'].map(pretrained_vectorizer_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82367a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 65.7%\n",
      "recall: 73.8%\n",
      "f1 score: 69.5%\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = tfidf_squad_dataset['predicted_labels']\n",
    "labels = tfidf_squad_dataset['labels']\n",
    "\n",
    "flat_predicted_labels = [predicted_label for predicted_unit in predicted_labels for predicted_label in predicted_unit]\n",
    "flat_labels = [label for label_unit in labels for label in label_unit]\n",
    "\n",
    "score_precision = precision_score(flat_predicted_labels, flat_labels)\n",
    "score_recall = recall_score(flat_predicted_labels, flat_labels)\n",
    "score_f1 = f1_score(flat_predicted_labels, flat_labels)\n",
    "\n",
    "print(f'precision: {100*score_precision:.1f}%')\n",
    "print(f'recall: {100*score_recall:.1f}%')\n",
    "print(f'f1 score: {100*score_f1:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3588c",
   "metadata": {},
   "source": [
    "We want to finetune this model to see if we can improve the results on the QA labeling context task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e88d1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet approach with the context, the question and the labels\n",
    "df_train = squad_dataset['train']\n",
    "cut_context_train = df_train['cut_context']\n",
    "question_train = df_train['question']\n",
    "labels_train = df_train['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9259b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_for_train = []\n",
    "for i in range(len(question_train)):\n",
    "    for _ in range(len(cut_context_train[i])):\n",
    "        question_for_train.append(question_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe5c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_context_for_train = [cut_context for cut_context_unit in cut_context_train for cut_context in cut_context_unit]\n",
    "labels_for_train = [label for label_unit in labels_train for label in label_unit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dff3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_training = []\n",
    "\n",
    "for index in range(len(cut_context_for_train)):\n",
    "    new_row = {\n",
    "        'question': question_for_train[index],\n",
    "        'context': cut_context_for_train[index],\n",
    "        'label': labels_for_train[index]\n",
    "    }\n",
    "    data_for_training.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b1c6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [InputExample(texts=[data['question'], data['context']], label=float(data['label'])) for data in data_for_training]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4307cb",
   "metadata": {},
   "source": [
    "We have generated a training dataset for the purpose of fine-tuning our Sentence Transformer model. In this process, we will employ the Cosine Similarity loss function. The input pairs consist of two sentences, representing a question and its corresponding context. The model's objective is to predict whether these two sentences are similar (indicating that the answer to the question is present in the context, labeled as 1) or dissimilar (labeled as 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5a09602",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentencesDataset(train_examples, model=model)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35c922f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daf7c8891e84c90b69b9f0403435db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e2c61457f94ec78491638c5988d508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a838bde7884ccfa69907cc2f186cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a8e379d0db403290d685cd199ededc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, output_path='fine_tuned_model_squad')\n",
    "\n",
    "model.save('fine_tuned_model_squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83a5ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = SentenceTransformer('fine_tuned_model_squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daf49e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_vectorizer_predict_label(dataset):\n",
    "\n",
    "    bert_encoded_cut_contexts = model.encode(dataset['cut_context'])\n",
    "    bert_encoded_questions = model.encode(dataset['question'])\n",
    "\n",
    "    similarities = [cosine_similarity([bert_encoded_questions], [encoded_cut_context])[0][0] for encoded_cut_context in bert_encoded_cut_contexts]\n",
    "\n",
    "    index_with_most_similarity = np.argmax(similarities)\n",
    "\n",
    "    predicted_labels = [1 if i == index_with_most_similarity else 0 for i in range(len(similarities))]\n",
    "\n",
    "    return {'index_with_most_similarity': index_with_most_similarity, 'predicted_labels': predicted_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "802df49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076cb1e27a7a422d82a4bcf83031783d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_vectorizer_squad_dataset = squad_dataset['validation'].map(optim_vectorizer_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3cb1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = finetuned_vectorizer_squad_dataset['predicted_labels']\n",
    "labels = finetuned_vectorizer_squad_dataset['labels']\n",
    "\n",
    "flat_predicted_labels = [predicted_label for predicted_unit in predicted_labels for predicted_label in predicted_unit]\n",
    "flat_labels = [label for label_unit in labels for label in label_unit]\n",
    "\n",
    "finetuned_score_precision = precision_score(flat_predicted_labels, flat_labels)\n",
    "finetuned_score_recall = recall_score(flat_predicted_labels, flat_labels)\n",
    "finetuned_score_f1 = f1_score(flat_predicted_labels, flat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6ed2263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Fine tuned model -------------\n",
      "Fine tuned precision: 74.3%\n",
      "Fine tuned recall: 83.4%\n",
      "Fine tuned f1 score: 78.6%\n",
      "\n",
      "------------- Base model -------------\n",
      "Basic precision: 65.7%\n",
      "Basic recall: 73.8%\n",
      "Basic f1 score: 69.5%\n"
     ]
    }
   ],
   "source": [
    "print('------------- Fine tuned model -------------')\n",
    "print(f'Fine tuned precision: {100*finetuned_score_precision:.1f}%')\n",
    "print(f'Fine tuned recall: {100*finetuned_score_recall:.1f}%')\n",
    "print(f'Fine tuned f1 score: {100*finetuned_score_f1:.1f}%')\n",
    "\n",
    "print('')\n",
    "\n",
    "print('------------- Base model -------------')\n",
    "print(f'Basic precision: {100*score_precision:.1f}%')\n",
    "print(f'Basic recall: {100*score_recall:.1f}%')\n",
    "print(f'Basic f1 score: {100*score_f1:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436ee2c",
   "metadata": {},
   "source": [
    "Upon evaluating both models, we observed that fine-tuning on the SQuAD dataset enhanced the model's performance in the context of the question-answering task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d3698",
   "metadata": {},
   "source": [
    "## 3 - Local search on another dataset: does it work ? \n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Let's implement our local semantic search on another dataset, to check if performance follows the same trend. You can use the [```commonsense_qa``` dataset](https://huggingface.co/datasets/commonsense_qa). Do the same exploration and explanation you did for the SQuAD task. How is this dataset different ? \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "Look at the data and apply the same two approaches you did before. What do you observe ? Propose an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b627faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f15f2da2e0d438a9f840bc7958d1461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245d2d8f55e144dab1f76aba6c263796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae6c1b0a80a476b8a6ebe32c891bfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset commonsense_qa/default to /Users/Guerlain/.cache/huggingface/datasets/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b6b6a2c9a9496493eddbb455b4d4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe028d1c4654cc0a8db3fbb41775470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2caa12caa146219b20733f75f120ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/472k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d19b0167bfe4dc28a20ac89c4e5a69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/423k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a32be3033b40269fe23d8da472629e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe0f77ccc05444bb16b56eb380e70d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18466e8cd1c348caa4c4919291e6bdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13998e51383a4d838be7a1f1154c7424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset commonsense_qa downloaded and prepared to /Users/Guerlain/.cache/huggingface/datasets/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca4e17cc2944d1f89587eb6ff188f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cqa_dataset = load_dataset(\"commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efd081d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'context', 'question', 'answers', 'cut_context', 'labels'])\n",
      "dict_keys(['id', 'question', 'question_concept', 'choices', 'answerKey'])\n"
     ]
    }
   ],
   "source": [
    "print(squad_dataset['validation'].features.keys())\n",
    "print(cqa_dataset['validation'].features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac53458",
   "metadata": {},
   "source": [
    "The dataset is designed to address questions by selecting from various answer options. It comprises questions with multiple potential answers and includes a conceptual component to provide additional context cues to the model. Unlike the SQuAD dataset, where the model extracts information from a given text to answer the question, this task relies on the model's commonsense knowledge. Consequently, the dataset offers only a single word of context to assist the model, as opposed to an entire text containing the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3395f7b",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6fa1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_predict_label(dataset):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit([dataset['question'] + ' ' + dataset['question_concept'] + ' ' + ' '.join(dataset['choices']['text'])])\n",
    "\n",
    "    tfidf_vectorized_questions = vectorizer.transform([dataset['question']]).toarray()\n",
    "    tfidf_vectorized_choices = vectorizer.transform(dataset['choices']['text']).toarray()\n",
    "\n",
    "    similarities = [cosine_similarity(tfidf_vectorized_questions, tfidf_vectorized_choice.reshape(1, -1))[0][0] for tfidf_vectorized_choice in tfidf_vectorized_choices]\n",
    "\n",
    "    index_with_most_similarity = np.argmax(similarities)\n",
    "\n",
    "    predicted_answer = dataset['choices']['label'][index_with_most_similarity]\n",
    "\n",
    "    return {'index_with_most_similarity': index_with_most_similarity, 'predicted_answer': predicted_answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63c919e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686166a104e444d1bb85262e7319982c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf_cqa_dataset = cqa_dataset['validation'].map(tfidf_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "075ff3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 20.1%\n"
     ]
    }
   ],
   "source": [
    "predictions = tfidf_cqa_dataset['predicted_answer']\n",
    "labels = tfidf_cqa_dataset['answerKey']\n",
    "\n",
    "score_accuracy = accuracy_score(predictions, labels)\n",
    "\n",
    "print(f'Accuracy: {100*score_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f04591",
   "metadata": {},
   "source": [
    "### Pre-trained vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfe72b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('nq-distilbert-base-v1')\n",
    "\n",
    "def pretrained_vectorizer_predict_label(dataset):\n",
    "\n",
    "    bert_encoded_choices = model.encode(dataset['choices']['text'])\n",
    "    bert_encoded_questions = model.encode(dataset['question'] + ' ' + dataset['question_concept'])\n",
    "\n",
    "    similarities = [cosine_similarity([bert_encoded_questions], [bert_encoded_choice])[0][0] for bert_encoded_choice in bert_encoded_choices]\n",
    "\n",
    "    index_with_most_similarity = np.argmax(similarities)\n",
    "\n",
    "    predicted_answer = dataset['choices']['label'][index_with_most_similarity]\n",
    "\n",
    "    return {'index_with_most_similarity': index_with_most_similarity, 'predicted_answer': predicted_answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8ed01d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d33b1a6ebc5433c9ed8121d5269ff1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_vectorizer_cqa_dataset = cqa_dataset['validation'].map(pretrained_vectorizer_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc129bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.2%\n"
     ]
    }
   ],
   "source": [
    "predictions = pretrained_vectorizer_cqa_dataset['predicted_answer']\n",
    "labels = pretrained_vectorizer_cqa_dataset['answerKey']\n",
    "\n",
    "score_accuracy = accuracy_score(predictions, labels)\n",
    "\n",
    "print(f'Accuracy: {100*score_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ae759",
   "metadata": {},
   "source": [
    "In this dataset that demands common sense knowledge, it is observed that the Transformer model outperforms the alternative. This outcome aligns with expectations, given that the Transformer model is pre-trained, and as a consequence, the embeddings it produces for phrases and words encompass significant semantic and contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd8f0c",
   "metadata": {},
   "source": [
    "### Pretrained model - finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "881d3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach with the question, the question concept, the choices and the answer\n",
    "df_train = cqa_dataset['train']\n",
    "question_train = df_train['question']\n",
    "concept_train = df_train['question_concept']\n",
    "choices_train = df_train['choices']\n",
    "answers_train = df_train['answerKey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f96cc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_for_train = []\n",
    "concept_for_train = []\n",
    "for i in range(len(question_train)):\n",
    "    for _ in range(len(choices_train[i]['label'])):\n",
    "        question_for_train.append(question_train[i])\n",
    "        concept_for_train.append(concept_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40180b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_dict = {key: [choice[key] for choice in choices_train] for key in choices_train[0]}\n",
    "texts_for_train = [text for texts_unit in choices_dict['text'] for text in texts_unit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e0e550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels = []\n",
    "for i in range(len(answers_train)):\n",
    "    binary_labels.append([1 if label == answers_train[i] else 0 for label in choices_dict['label'][i]])\n",
    "\n",
    "labels_for_train = [label for labels_unit in binary_labels for label in labels_unit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed2d6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_training = []\n",
    "\n",
    "for i in range(len(question_train)):\n",
    "    new_row = {\n",
    "        'question_concept': question_train[i] + ' ' + concept_train[i],\n",
    "        'choices': choices_train[i],\n",
    "        'labels': labels_for_train[i]\n",
    "    }\n",
    "    data_for_training.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8c831a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [InputExample(texts=[data['question_concept'], data['choices']], label=float(data['labels'])) for data in data_for_training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e79515eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentencesDataset(train_examples, model=model)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49cba0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617c5c54bc3a4ceca697311a1a0533b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e5f9b8f4714768a95a739238077579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d46a85dda2440fd95333761b3707c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2bba6e9c6d495fb7498269507262d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, output_path='fine_tuned_model_cqa')\n",
    "\n",
    "model.save('fine_tuned_model_cqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba5b11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = SentenceTransformer('fine_tuned_model_cqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d078c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_vectorizer_predict_label(dataset):\n",
    "\n",
    "    bert_encoded_choices = finetuned_model.encode(dataset['choices']['text'])\n",
    "    bert_encoded_questions = finetuned_model.encode(dataset['question'] + ' ' + dataset['question_concept'])\n",
    "\n",
    "    similarities = [cosine_similarity([bert_encoded_questions], [bert_encoded_choice])[0][0] for bert_encoded_choice in bert_encoded_choices]\n",
    "\n",
    "    index_with_most_similarity = np.argmax(similarities)\n",
    "\n",
    "    predicted_answer = dataset['choices']['label'][index_with_most_similarity]\n",
    "\n",
    "    return {'index_with_most_similarity': index_with_most_similarity, 'predicted_answer': predicted_answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "525f81a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d4f2487f674f6b9a1ea1eb753288f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_vectorizer_cqa_dataset = cqa_dataset['validation'].map(optim_vectorizer_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97a1a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answers = finetuned_vectorizer_cqa_dataset['predicted_answer']\n",
    "labels = finetuned_vectorizer_cqa_dataset['answerKey']\n",
    "\n",
    "finetuned_score_accuracy = accuracy_score(predicted_answers, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "82bb12cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 31.7%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {100*finetuned_score_accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3ff9e",
   "metadata": {},
   "source": [
    "We should increase the number of epochs to reach a better fine-tuned model. We chose 3 epochs because our computation lasts for 30 minutes with only 3 epochs so we are ressourced-llimited to make bigger improvements on our task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ed206",
   "metadata": {},
   "source": [
    "## 4 - Global search on Wikipedia data\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Again, look at the data of the [```wiki_qa``` dataset](https://huggingface.co/datasets/wiki_qa), understand the task. We are now going to perform a **global** search, as the dataset is open domain: when trying to answer for a question, we will search among all vectors, rather than only the ones representing the context the answer is found in. How would you verify that the model managed to find the right answer ? Let's try to use to very different ways to evaluate how well the approaches work:\n",
    "- Looking if the right result is in the top-$k$ predictions returned by the model.\n",
    "- Using the [ROUGE](https://aclanthology.org/W04-1013/) score. \n",
    "Explain how you understand these metrics and how they could be useful here.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "We will use the same embeddings as before, but we will use a tool called ```faiss``` for indexing all of them and facilitate the search ! Look at the [documentation](https://huggingface.co/docs/datasets/faiss_es). Then, implement or use tools implementing the two metrics, and evaluate both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "36430569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2a0ff73e4349d880308e5bf2524d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eae837f695f4a008179b8538798fac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf2e0c4848647029fcf6eeb6f001b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/13.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wiki_qa/default to /Users/Guerlain/.cache/huggingface/datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6a63258d674143be22ccea84e50472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fd1303237d46b0ae10998c8b89ffdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa655b7b4872487aa161ef60a739835e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b015a359888e4bd5b899c9b7e7312ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wiki_qa downloaded and prepared to /Users/Guerlain/.cache/huggingface/datasets/wiki_qa/default/0.1.0/d2d236b5cbdc6fbdab45d168b4d678a002e06ddea3525733a24558150585951c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b203ebceb94980bd30559cdeb7020f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_dataset = load_dataset('wiki_qa')\n",
    "wiki_test = wiki_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f8d29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('nq-distilbert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e08478b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [elem['answer'] for elem in wiki_dataset['train']]\n",
    "\n",
    "bert_encoded_answers = np.array(model.encode(answers)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b318a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(bert_encoded_answers.shape[1])\n",
    "index.add(bert_encoded_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "44403555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_global_search(question, k=0):\n",
    "\n",
    "    bert_encoded_question = np.array(model.encode(question)).astype('float32').reshape(1,-1)\n",
    "\n",
    "    _, bound = index.search(bert_encoded_question, k)\n",
    "    answers_top_k = [answers[i] for i in bound[0]]\n",
    "\n",
    "    return answers_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "261f9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_base(question, answer, k=0):\n",
    "\n",
    "    answers_top_k = k_global_search(question, k)\n",
    "    \n",
    "    return answer in answers_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94f483f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(prediction, ground_truth):\n",
    "\n",
    "    scoring_module = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scoring_module.score(ground_truth, prediction)\n",
    "\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f040892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_val = wiki_dataset['validation']  \n",
    "\n",
    "correct_count, total_count, k = 0, 0, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6ef3912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in wiki_test:\n",
    "\n",
    "    question = row['question']\n",
    "    ground_truth = row['answer']\n",
    "\n",
    "    if row['label'] == 1:\n",
    "\n",
    "        if evaluate_base(question, ground_truth, k):\n",
    "            correct_count += 1\n",
    "            \n",
    "        total_count += 1\n",
    "\n",
    "accuracy = correct_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0f0d827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 accuracy: 9.2%\n"
     ]
    }
   ],
   "source": [
    "print(f'Top-{k} accuracy: {100*accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b399d828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how big is bmc software in houston, tx\n",
      "Prediction: It is based in Houston, Texas .\n",
      "Ground truth: Employing over 6,000, BMC is often credited with pioneering the BSM concept as a way to help better align IT operations with business needs.\n",
      "ROUGE scores: {'rouge1': Score(precision=0.3333333333333333, recall=0.08, fmeasure=0.12903225806451613), 'rougeL': Score(precision=0.16666666666666666, recall=0.04, fmeasure=0.06451612903225806)}\n",
      "\n",
      "Question: how big is bmc software in houston, tx\n",
      "Prediction: It is based in Houston, Texas .\n",
      "Ground truth: For 2011, the company recorded an annual revenue of $2.1 billion, making it the #20 largest software company in terms of revenue for that year.\n",
      "ROUGE scores: {'rouge1': Score(precision=0.3333333333333333, recall=0.07692307692307693, fmeasure=0.125), 'rougeL': Score(precision=0.3333333333333333, recall=0.07692307692307693, fmeasure=0.125)}\n",
      "\n",
      "Question: how long was i love lucy on the air\n",
      "Prediction: Not in Top K result\n",
      "Ground truth: I Love Lucy is an American television sitcom starring Lucille Ball , Desi Arnaz , Vivian Vance , and William Frawley .\n",
      "ROUGE scores: N/A\n",
      "\n",
      "Question: how long was i love lucy on the air\n",
      "Prediction: The show ran for 5 seasons between 2003 and 2005.\n",
      "Ground truth: The black-and-white series originally ran from October 15, 1951, to May 6, 1957, on the Columbia Broadcasting System (CBS).\n",
      "ROUGE scores: {'rouge1': Score(precision=0.3, recall=0.14285714285714285, fmeasure=0.19354838709677416), 'rougeL': Score(precision=0.2, recall=0.09523809523809523, fmeasure=0.12903225806451613)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for element in wiki_val.select(range(12,16)):\n",
    "\n",
    "    question = element['question']\n",
    "    ground_truth = element['answer'] \n",
    "    prediction = k_global_search(question, k=5)[0] if element['label'] == 1 else 'Not in Top K result'\n",
    "    \n",
    "    rouge_scores = evaluate_rouge(prediction, ground_truth) if element['label'] == 1 else 'N/A'\n",
    "    \n",
    "    print(f'Question: {question}')\n",
    "    print(f'Prediction: {prediction}')\n",
    "    print(f'Ground truth: {ground_truth}')\n",
    "    print(f'ROUGE scores: {rouge_scores}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e2bc0",
   "metadata": {},
   "source": [
    "Leveraging ROUGE scores has enabled us to gauge the degree of alignment between the content predicted in the response and the correct answer. Upon scrutinizing our examples, it becomes apparent that the ROUGE scores exhibit a degree of deficiency. This implies a constrained similarity between the predicted responses and the correct answers, both concerning individual words (ROUGE-1) and the sequence/order of words (ROUGE-L).\n",
    "\n",
    "These scores function constitute a valuable metric for assessing our model's proficiency in encapsulating the fundamental content of correct answers. Diminished scores may potentially indicate challenges, such as the model misconstruing the question or falling short in retrieving pertinent information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
