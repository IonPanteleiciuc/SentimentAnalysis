{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b17c85",
   "metadata": {},
   "source": [
    "# Lab: Semantic search on Question Answering datasets\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "1. Explore and understand the **S**tanford **Qu**estion **A**nswering **D**ataset [Squad](https://aclanthology.org/D16-1264/) dataset and the associated task.  \n",
    "2. Adapt this dataset for a *local* semantic search task and propose an appropriate evaluation metric:\n",
    "    - Implement a simple baseline based on **TF-IDF**.\n",
    "    - Use a pre-trained transformer-based model, and fine-tune it.\n",
    "3. Test these approaches on the [CommonSense QA](https://aclanthology.org/N19-1421/) dataset. \n",
    "4. Adapt these approaches for a *global* semantic search task on the [WikiQA](https://aclanthology.org/D15-1237/) dataset for open domain question answering.\n",
    "5. **Bonus** (Optional) Apply a model (any, as long as it's running) to the original Squad QA task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409670c",
   "metadata": {},
   "source": [
    "## Modalities:\n",
    "\n",
    "The goal of this lab is to make you search for and learn to use recent tools for NLP tasks. \n",
    "- You should feel free to use any tool, implementation and model you prefer. \n",
    "- You are not expected to reach a particular performance. \n",
    "- You can work on this lab by groups of up to 3. \n",
    "- You should submit this lab on the Moodle by Friday 22th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b8ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from datasets import load_dataset\n",
    "# new comment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de1dc6",
   "metadata": {},
   "source": [
    "## 1 - The SQuAD dataset\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Questions:</div>\n",
    "\n",
    "1. Load the dataset **SQuAD** - for example, using the [```dataset``` package](https://huggingface.co/docs/datasets/index) from Huggingface and loading the dataset ```'squad'```. You can also explore it using the [website](https://rajpurkar.github.io/SQuAD-explorer/). \n",
    "2. Look at the metrics used to evaluate models on the dataset. You can also load the metric ```'squad'``` from the [```evaluate``` package](https://huggingface.co/docs/evaluate/index) from Huggingface. \n",
    "3. Explain succintly - and in your own words - what is the task: how could we use a model to solve it ? Treat the case of encoder models adapted to *classification tasks* and encoder-decoder models adapted to *text generation*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/Guerlain/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6e4d13cd2d4177be75290eece9a66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset = load_dataset('squad')\n",
    "df_train = squad_dataset['train']\n",
    "df_valid = squad_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd4da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/Guerlain/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf46e4afca54fdfb3a5c6c5368552c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/Guerlain/Documents/X/NLP/Labs/Lab5/Lab_QA_DS_Telecom_20-Students.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Guerlain/Documents/X/NLP/Labs/Lab5/Lab_QA_DS_Telecom_20-Students.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m squad_dataset \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(load_dataset(\u001b[39m'\u001b[39m\u001b[39msquad\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Guerlain/Documents/X/NLP/Labs/Lab5/Lab_QA_DS_Telecom_20-Students.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m squad_dataset\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy, typ\u001b[39m=\u001b[39mmanager)\n\u001b[1;32m    710\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39mdtype, typ\u001b[39m=\u001b[39mtyp, consolidate\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    116\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:655\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    657\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    658\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    659\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "squad_dataset = pd.DataFrame(load_dataset('squad'))\n",
    "squad_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba300531",
   "metadata": {},
   "source": [
    "## 2 - Design a *local* semantic search from squad\n",
    "\n",
    "This taks is a little complicated to implement. Let us simplify squad to be a **semantic search** task !\n",
    "We will divide the context containing the answer into several pieces, and ask a model to find which one contains the answer **by vectorizing the question and each piece** and trying to look for the most relevant piece using **cosine similarity** between the vectors, making it a fairly simple task.\n",
    "\n",
    "\n",
    "For example, the following question of the dataset:\n",
    "\n",
    "```python\n",
    "'Which NFL team represented the AFC at Super Bowl 50?'\n",
    "```\n",
    "\n",
    "with the answer:\n",
    "\n",
    "```python\n",
    "'Denver Broncos'\n",
    "```\n",
    "\n",
    "We could divide the corresponding ```'context'``` into the following list:\n",
    "\n",
    "```python\n",
    "['Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos d',\n",
    " \"efeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Franc\",\n",
    " 'isco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending',\n",
    " ' the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.']\n",
    "```\n",
    "\n",
    "and indicate the location of the answer as:\n",
    "\n",
    "```python\n",
    "label = [1, 0, 0, 0]\n",
    "```\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "At first, we won't do any training: you should work with the ```validation``` part of the dataset. Be careful, there may be several good answers ! Propose a scheme to divide the context into pieces and to label each piece as containing the answer or not. How do we evaluate for this task - would simple accuracy suffice ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160c68f",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "For efficient processing, you can use the ```map``` method associated to the dataset. It can create a new feature for each example. In this case, you can create a new feature containing the context divided into pieces, and a new feature containing labels for if the pieces contain the answer. You can also use it for your evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ee657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4df0d5ef",
   "metadata": {},
   "source": [
    "### 2.1 - Local search: Independant Tf-idf representations\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "Implement a function that will for each example:\n",
    "- Create a tf-idf ```vectorizer``` from all the text in the question and context. \n",
    "- Create tf-idf representations for the question and the pieces of the context,\n",
    "- Find the representation the closest to the question among the pieces.\n",
    "\n",
    "Then, evaluate the method !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a523c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44498817",
   "metadata": {},
   "source": [
    "### 2.2 - Local search: Pre-trained sentence representations transformer-based model\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "\n",
    "Reproduce the same process using a pre-trained transformer model. You can use a model that you will find on huggingface. You can also look into the [```SentenceTransformer``` library](https://www.sbert.net/), dedicated to represent documents. Also:\n",
    "- Try to verify if the model has been trained on SQuAD !\n",
    "- Fine-tune the model (at least a little) to check that it improves results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ea350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8d3698",
   "metadata": {},
   "source": [
    "## 3 - Local search on another dataset: does it work ? \n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Let's implement our local semantic search on another dataset, to check if performance follows the same trend. You can use the [```commonsense_qa``` dataset](https://huggingface.co/datasets/commonsense_qa). Do the same exploration and explanation you did for the SQuAD task. How is this dataset different ? \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "Look at the data and apply the same two approaches you did before. What do you observe ? Propose an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e30e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "569ed206",
   "metadata": {},
   "source": [
    "## 4 - Global search on Wikipedia data\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Again, look at the data of the [```wiki_qa``` dataset](https://huggingface.co/datasets/wiki_qa), understand the task. We are now going to perform a **global** search, as the dataset is open domain: when trying to answer for a question, we will search among all vectors, rather than only the ones representing the context the answer is found in. How would you verify that the model managed to find the right answer ? Let's try to use to very different ways to evaluate how well the approaches work:\n",
    "- Looking if the right result is in the top-$k$ predictions returned by the model.\n",
    "- Using the [ROUGE](https://aclanthology.org/W04-1013/) score. \n",
    "Explain how you understand these metrics and how they could be useful here.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "            \n",
    "We will use the same embeddings as before, but we will use a tool called ```faiss``` for indexing all of them and facilitate the search ! Look at the [documentation](https://huggingface.co/docs/datasets/faiss_es). Then, implement or use tools implementing the two metrics, and evaluate both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f261e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bdc4241",
   "metadata": {},
   "source": [
    "## 5 - BONUS: Run a model on the original Squad task\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>\n",
    "\n",
    "Of course, we need to know that you understood the code: simplify it to the maximum (only what's necessary to obtain predictions) and comment abundantly ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c70c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
